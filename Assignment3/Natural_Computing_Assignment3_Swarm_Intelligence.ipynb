{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Computing Assignment 3 \n",
    "Simge Ekiz(s4706757), Luca Parola(s1009497), Katrin Bujari(s1005213)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Consider an illustrative example of a PSO system composed of three particles. Consider the following update rule for each particle i and dimension d:\n",
    "\\begin{equation}\n",
    "\t\tv(i;d) = wv(i;d) + r_1({x}^*(i;d) - x(i;d)) + r_2({x}^*(d) - x(i;d)) \n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "To facilitate calculation, we will ignore the fact that $r_1$ and $r_2$ are random numbers and fix them to $0.5$ for this exercise. The space of solutions is the two dimensional real valued space and the current state of the swarm is as follows\n",
    "\n",
    "-  Position of particles: $x_1 = (5,5); x_2 = (8,3); x_3 = (6,7)$\n",
    "-  Individual best positions: $x^*_1 = (5,5); x^*_2 = (7,3); x^*_3 = (5,6)$\n",
    "-  Social best position: $x^* = (5,5)$\n",
    "-  Velocities: $v_1 = (2,2); v_2 = (3,3); v_3 = (4,4)$\n",
    "\n",
    "\n",
    "a-) What would be the next position of each particle after one iteration of the PSO algorithm with w = 2?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding-left: 20px;\">\n",
    "$w = 2$\n",
    "$r_1 = 0.5$\n",
    "$r_2 = 0.5$\n",
    "<br><br>\n",
    "<b> According to formula; </b><br>\n",
    "  <span style=\"padding-left: 15px;\" >  $v(i;d) = wv(i;d) + r_1(x^∗(i;d) - x(i;d)) + r2(x^∗(d) - x(i;d))$\n",
    "  </span>\n",
    "<br><br>\n",
    "<b> Velocity of particle 1; </b><br> \n",
    "<span style=\"padding-left: 15px;\" >\n",
    "    $v(1;xaxis) = 2*2 + 0.5*(5-5) + 0.5*(5-5) = 4$ <br>\n",
    "</span>\n",
    "<span style=\"padding-left: 15px;\" >\n",
    "    $v(1;yaxis) = 2*2 + 0.5*(5-5) + 0.5*(5-5) = 4$ <br> \n",
    "</span>\n",
    "<span style=\"padding-left: 15px;\" >\n",
    "    ${v}_1^2 = (4, 4)$ \n",
    "</span>\n",
    "<br><br>\n",
    "<b> Velocity of particle 2;</b> <br>\n",
    "<span style=\"padding-left: 15px;\" >\n",
    "    $v(2;xaxis) = 2*3 + 0.5*(7-8) + 0.5*(5-8) = 4$ <br>\n",
    "</span>\n",
    "<span style=\"padding-left: 15px;\" >\n",
    "    $v(2;yaxis) = 2*3 + 0.5*(3-3) + 0.5*(5-3) = 7$ <br>\n",
    "</span>\n",
    "<span style=\"padding-left: 15px;\" >\n",
    "    $v_2^2 = (4, 7)$ \n",
    "</span>\n",
    "<br><br>\n",
    "<b> Velocity of particle 3; </b><br>\n",
    "<span style=\"padding-left: 15px;\" >\n",
    "    $v(3;xaxis) = 2*4 + 0.5*(5-6) + 0.5*(5-6) = 7$ <br>\n",
    "</span>\n",
    "<span style=\"padding-left: 15px;\" >\n",
    "    $v(3;yaxis) = 2*4 + 0.5*(6-7) + 0.5*(5-7) = 6.5$ <br>\n",
    "</span>\n",
    "<span style=\"padding-left: 15px;\" >\n",
    "    $v_3^2 = (7, 6.5)$ \n",
    "</span>\n",
    "<br><br>\n",
    "<b> Updating positions : </b> <br>\n",
    "<span style=\"padding-left: 15px;\" >\n",
    "    $x_i^{t+1} = x_i^{t} + v_i^{t+1}$\n",
    "</span><br><br>\n",
    "<b> Position of particle 1; </b> <br>\n",
    "<span style=\"padding-left: 15px;\" >\n",
    "$x_1^2 = (5, 5) + v_1^2 = (5, 5) + (4, 4) = (9, 9) $\n",
    "</span>\n",
    "<br><br>\n",
    "<b> Position of particle 2; </b><br>\n",
    "<span style=\"padding-left: 15px; \" >\n",
    "$x_2^2 = (8,3) + v_2^2 = (8, 3) + (4, 7) = (12, 10) $ \n",
    "</span> <br><br>\n",
    "<b>Position of particle 3; </b> <br>\n",
    "<span style=\"padding-left: 15px;\" >\n",
    "$x_3^2 = (6,7) + v_3^2 = (6, 7) + (7, 6.5) = (13, 13.5) $ \n",
    "</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b-) And using w = 0.1?\n",
    "\n",
    "<div style=\"padding: 20px;\">\n",
    "$w=0.1$\n",
    "$r_1 = 0.5$\n",
    "$r_2 = 0.5$\n",
    "<br><br>\n",
    "<b>Velocity of particle 1;</b> <br> \n",
    "<span style=\"padding-left: 15px;\" >\n",
    "    $v(1;xaxis) = 0.1*2 + 0.5*(5-5) + 0.5*(5-5) = 0.2$ <br>\n",
    "</span>\n",
    "<span style=\"padding-left: 15px;\" >\n",
    "    $v(1;yaxis) = 0.1*2 + 0.5*(5-5) + 0.5*(5-5) = 0.2$ <br> \n",
    "</span>\n",
    "<br>\n",
    "<b>Velocity of particle 2;</b> <br>\n",
    "<span style=\"padding-left: 15px;\" >\n",
    "    $v(2;xaxis) = 0.1*3 + 0.5*(7-8) + 0.5*(5-8) = -1.7$ <br>\n",
    "</span>\n",
    "<span style=\"padding-left: 15px;\" >\n",
    "    $v(2;yaxis) = 0.1*3 + 0.5*(3-3) + 0.5*(5-3) = 1.3$ <br>\n",
    "</span>\n",
    "<br>\n",
    "<b>Velocity of particle 3;</b> <br>\n",
    "<span style=\"padding-left: 15px;\" >\n",
    "    $v(3;xaxis) = 0.1*4 + 0.5*(5-6) + 0.5*(5-6) = -0.6$ <br>\n",
    "</span>\n",
    "<span style=\"padding-left: 15px;\" >\n",
    "    $v(3;yaxis) = 0.1*4 + 0.5*(6-7) + 0.5*(5-7) = -1.1$ <br>\n",
    "</span>\n",
    "<br><br>\n",
    "\n",
    "<b>Position of particle 1;</b> <br>\n",
    "<span style=\"padding-left: 15px;\" >\n",
    "$x_1^2 = (5, 5) + (0.2, 0.2) = (5.2, 5.2) $\n",
    "</span>\n",
    "<br><br>\n",
    "<b>Position of particle 2;</b> <br>\n",
    "<span style=\"padding-left: 15px; \" >\n",
    "$x_2^2 = (8, 3) + (-1.7, 1.3) = (6.3, 4.3) $ \n",
    "</span> <br><br>\n",
    "<b>Position of particle 3; </b> <br>\n",
    "<span style=\"padding-left: 15px;\" >\n",
    "$x_3^2 = (6, 7) + (-0.6, -1.1) = (5.4, 5.9) $ \n",
    "</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c-) Explain what is the effect of the parameter w.\n",
    "> In PSO, for each particle, velocity is calculated according to its previous velocity, the particle location which has the best fitness so far and any particles location which has the best fitness so far. W carries a factor that how much the previous velocity contribute to the next velocity. small w facilitates local search, it exploits the current position as much as it can. Large w facilitates global search, it aims to explore solution space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d-) Give an advantage and a disadvantage of a high value of w.\n",
    "> An advantage of high value w is that it allows us to explore more. Particles move faster and more solution space could be searced and this increase the possibility of finding a better solution.\n",
    "A disadvantage of of high value w is that;\n",
    "it does less expoitation and gets less sensitive to new information in the influence from the current particle. Thus, it might skip over an optimum.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Consider a particle swarm consisting of a single member. How would it perform in a\n",
    "trivial task such as the minimization of $f(x) = x^2$ when $w < 1$?\n",
    "\n",
    "> Given that w<1 and $f(x) = x^2$ it would find the optimum solution. Finding a solution depends on the objective function since f(x) establishes a convex solution space which has less complexity of the search space. For instance, if the particle starts with the velocity which drawns the particle away from the optimum, the personal best affects the particle and after some time particle will be moving to an optimum point. When velocity is in the same direction with the optimum, personal best loses its influence. Then, velocity starts to decrease. If the particle reaches to optimum before the velocity is zero, algorithm finds the optimum point. Particle will find the optimum for this trivial problem, because when w < 1, particles would be moving around the optimum. Then, after some steps, velocity would be zero at the optimum.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "Implement the PSO algorithm for clustering described in “Van der Merwe, D. W., and Andries Petrus Engelbrecht. ”Data clustering using particle swarm optimization.” Evolutionary Computation, 2003. CEC’03. The 2003 Congress on. Vol. 1. IEEE, 2003.” (see also swarm intelligence slides). Implement the k-means clustering.\n",
    "Apply and compare the performance of the two algorithms in terms of quantization error on Artificial dataset 1 and Iris dataset (the latter available at UCI ML repository, see https://archive.ics.uci.edu/ml/datasets/iris). In both algorithms, use the true number of clusters as value of the parameter for setting the number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build the Artificial dataset defined as the equation 9 in Van der Merwe, D. W., and\n",
    "Andries Petrus Engelbrecht. \"Data clustering using particle swarm optimization.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1=np.random.uniform(-1,1,400)\n",
    "z2=np.random.uniform(-1,1,400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "artificial_df=pd.DataFrame({\"Z1\" : z1,\"Z2\" : z2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans1 = KMeans(n_clusters=2, random_state=0).fit(artificial_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We predict the class of each datapoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_artificial_df=kmeans1.predict(artificial_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compute the quantization error for the artificial dataset\n",
    "\n",
    "### quantization error\n",
    "\n",
    "$$J_{e}\\,=\\,{{\\sum_{j=1}^{N_{c}}[\\sum_{\\forall {\\bf z}_{{\\rm p}}\\in C_{ij}} d({\\bf z}_{p},{\\bf m}_{j})/\\vert C_{ij}\\vert]}\\over{N_{c}}}$$\n",
    "\n",
    "\n",
    "$$d ( {{\\bf z}_{p}, m_{j}})\\,=\\,\\sqrt{\\sum_{k=1}^{N_{j}}(z_{pk}-m_{jk})^{2}}$$\n",
    "\n",
    "\n",
    "\n",
    "$N_{c}$ denotes the number of cluster centroids (as provided by the user), i.e. the number of dusters to be formed\n",
    "\n",
    "${\\bf z}_{p}$ denotes the p-th data vector\n",
    "\n",
    "$C_{j}$ is the subset of data vectors that form cluster j.\n",
    "\n",
    "$m_{j}$ is the centroid \n",
    "\n",
    "$d_{{\\bf z}_{p}, m_{j}}$ is the Euclidean distance \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##we take the centroids:\n",
    "centroid1_A_df=kmeans1.cluster_centers_[0]\n",
    "centroid2_A_df=kmeans1.cluster_centers_[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we assign each data point to the predicted cluster \n",
    "cluster1_A=[]\n",
    "cluster2_A=[]\n",
    "for i in range(0,len(artificial_df)):\n",
    "    if prediction_artificial_df[i]==0:\n",
    "        \n",
    "        cluster1_A.append(artificial_df.loc[i])\n",
    "        \n",
    "    if prediction_artificial_df[i]==1:\n",
    "        cluster2_A.append(artificial_df.loc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantization_error2(cluster1,cluster2,centroid1,centroid2):\n",
    "            \n",
    "    distance1=0\n",
    "    distance2=0\n",
    "\n",
    "\n",
    "    for i in range(0,len(cluster1)):\n",
    "\n",
    "        dist1=scipy.spatial.distance.euclidean(np.asarray(cluster1[i]), centroid1)\n",
    "        distance1=distance1+dist1\n",
    "    \n",
    "    if len(cluster1)>0:\n",
    "        distance1=distance1/len(cluster1)\n",
    "    else:\n",
    "        distance1=0\n",
    "\n",
    "    for i in range(0,len(cluster2)):\n",
    "\n",
    "        dist2=scipy.spatial.distance.euclidean(np.asarray(cluster2[i]), centroid2)\n",
    "        distance2=distance2+dist2\n",
    "    \n",
    "    if len(cluster2)>0:\n",
    "        distance2=distance2/len(cluster2)\n",
    "    else:\n",
    "        distance2=0\n",
    "\n",
    "\n",
    "    QE=(distance1+distance2)/2\n",
    "    ##print(\"Quantization error: \",QE)\n",
    "    return QE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quantization error for the KMEANS algorithm for the artificial dataset is   0.5686478747859159\n"
     ]
    }
   ],
   "source": [
    "print (\"The quantization error for the KMEANS algorithm for the artificial dataset is  \", quantization_error2(cluster1_A,cluster2_A,centroid1_A_df,centroid2_A_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can do the same with the iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "y=iris.target\n",
    "iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans2 = KMeans(n_clusters=3, random_state=0).fit(iris_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_iris_df=kmeans2.predict(iris_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##we take the centroids:\n",
    "centroid1_iris=kmeans2.cluster_centers_[0]\n",
    "centroid2_iris=kmeans2.cluster_centers_[1]\n",
    "centroid3_iris=kmeans2.cluster_centers_[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we assign each data point to the predicted cluster \n",
    "cluster1_iris=[]\n",
    "cluster2_iris=[]\n",
    "cluster3_iris=[]\n",
    "for i in range(0,len(iris_df)):\n",
    "    if prediction_iris_df[i]==0:\n",
    "        cluster1_iris.append(iris_df.loc[i])\n",
    "    if prediction_iris_df[i]==1:\n",
    "        cluster2_iris.append(iris_df.loc[i])\n",
    "    if prediction_iris_df[i]==2:\n",
    "        cluster3_iris.append(iris_df.loc[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our qunatization error function for 3 cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantization_error3(cluster1,cluster2,cluster3,centroid1,centroid2,centroid3):\n",
    "            \n",
    "    distance1=0\n",
    "    distance2=0\n",
    "    distance3=0\n",
    "    count=0\n",
    "    for i in range(0,len(cluster1)):\n",
    "\n",
    "        dist1=scipy.spatial.distance.euclidean(np.asarray(cluster1[i]), centroid1)\n",
    "        distance1=distance1+dist1\n",
    "    \n",
    "    if len(cluster1)>0:\n",
    "        distance1=distance1/len(cluster1)\n",
    "    else:\n",
    "        distance1=0\n",
    "    \n",
    "   \n",
    "    \n",
    "\n",
    "    for i in range(0,len(cluster2)):\n",
    "\n",
    "        dist2=scipy.spatial.distance.euclidean(np.asarray(cluster2[i]), centroid2)\n",
    "        distance2=distance2+dist2\n",
    "    \n",
    "    if len(cluster2)>0:\n",
    "        distance2=distance2/len(cluster2)\n",
    "    else:\n",
    "        distance2=0\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "    for i in range(0,len(cluster3)):\n",
    "\n",
    "        dist3=scipy.spatial.distance.euclidean(np.asarray(cluster3[i]), centroid3)\n",
    "        distance3=distance3+dist3\n",
    "    \n",
    "    if len(cluster3)>0:\n",
    "        distance3=distance3/len(cluster3)\n",
    "    else:\n",
    "        distance3=0\n",
    "\n",
    "\n",
    "    \n",
    "    QE=(distance1+distance2+distance3)/3\n",
    "    return QE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quantization error for the KMEANS algorithm for the iris dataset is   0.6473743892616001\n"
     ]
    }
   ],
   "source": [
    "print (\"The quantization error for the KMEANS algorithm for the iris dataset is  \", quantization_error3(cluster1_iris,cluster2_iris,cluster3_iris,centroid1_iris,centroid2_iris,centroid3_iris))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can build the PSO algorithm and use it with the Artificial DATaset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we initialize 10 particles with 2 centroid each to use in the Artificial dataset\n",
    "particles_list=[]\n",
    "for i in range(0,10):\n",
    "    particles=np.random.uniform(-1,1,4).reshape(2,2)\n",
    "    particles_list.append(particles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After  9  iteration , the global best error is:  0.37917153606964155 best  swarn has positions:  [[ 0.64776438 -4.26549274]\n",
      " [ 0.06790201 -0.10070954]]\n"
     ]
    }
   ],
   "source": [
    "##we set our initial values\n",
    "\n",
    "personal_best_error=np.ones(10)\n",
    "global_best_position=[]\n",
    "global_best_error=1\n",
    "personal_best_position=np.zeros(40).reshape(10,2,2)\n",
    "\n",
    "alpha=1.49618\n",
    "w=0.7298\n",
    "velocity=np.zeros(4).reshape(2,2) \n",
    "\n",
    "##for each iteration\n",
    "for iteration in range(0,10):\n",
    "    \n",
    "    quantization_error=[] \n",
    "    count=0\n",
    "    \n",
    "    ##for each particles \n",
    "    for i in range(0,len(particles_list)):\n",
    "\n",
    "        \n",
    "        current_particle=particles_list[i]\n",
    "        cluster1=[]\n",
    "        cluster2=[]\n",
    "        \n",
    "       ##for each data point in the Artificial dataset\n",
    "        for a in range(0,len(artificial_df)):\n",
    "            \n",
    "            \n",
    "            ##we measure the euclidean distance from each centroid\n",
    "            dist1=scipy.spatial.distance.euclidean(np.asarray(artificial_df.loc[a]), current_particle[0])\n",
    "            dist2=scipy.spatial.distance.euclidean(np.asarray(artificial_df.loc[a]), current_particle[1])\n",
    "\n",
    "            ##we assign each datapoint to a cluster based on the closest centroid\n",
    "            if dist1<dist2:\n",
    "                cluster1.append(artificial_df.loc[a])\n",
    "\n",
    "\n",
    "            elif dist1>dist2:\n",
    "                cluster2.append(artificial_df.loc[a])\n",
    "\n",
    "\n",
    "        \n",
    "        ##Once we have our cluster we compute the quantization error and we store it\n",
    "        ## we will have a list with all our quantization error\n",
    "        quantization_error.append(quantization_error2(cluster1,cluster2,current_particle[0],current_particle[1]))\n",
    "\n",
    "   \n",
    "    \n",
    "    ##Then for each quantization error in the list\n",
    "    for z in range(0,len(quantization_error)):\n",
    "        count=count+1    \n",
    "        \n",
    "        ##if the quantization error of the current iteration is smaller than the personal best , we store it as new personal best\n",
    "        if quantization_error[z]<personal_best_error[z]:\n",
    "            personal_best_error[z]=quantization_error[z]\n",
    "            ##and we store the corresponding particle as new best position\n",
    "            personal_best_position[z]=particles_list[z]\n",
    "\n",
    "        ##we also update the best global error if we find a new best \n",
    "        if personal_best_error[z]<global_best_error:\n",
    "            global_best_error=personal_best_error[z]\n",
    "            global_best_position=particles_list[z]\n",
    "            \n",
    "    ###print (\"Particle list BEFORE, \", particles_list)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ## then for each particle we update the position of the centroids based on the formula\n",
    "    \n",
    "    for i in range(0,len(particles_list)):\n",
    "        \n",
    "        ##I've disassembled the formula for clarity.\n",
    "\n",
    "        \n",
    "\n",
    "        ##we set the random r\n",
    "        r1=np.random.uniform(0,1,1)\n",
    "        r2=np.random.uniform(0,1,1)\n",
    "\n",
    "        ##we compute the first term of the formula\n",
    "        first_term=np.multiply(w,velocity)\n",
    "        \n",
    "        ##we compute the two multiplication of the alpha and the r\n",
    "        alphaR1=alpha*r1\n",
    "        alphaR2=alpha*r2\n",
    "        \n",
    "        #we copmute the 2 subtractions\n",
    "        first_subtraction=np.subtract(personal_best_position[i],particles_list[i])\n",
    "        second_subtraction=np.subtract(global_best_position,particles_list[i])\n",
    "\n",
    "        #we compute the second term\n",
    "        second_term=np.multiply(alphaR1,first_subtraction)\n",
    "        \n",
    "        #we compute the second term\n",
    "        third_term=np.multiply(alphaR2,second_subtraction)\n",
    "        \n",
    "        ##we compute the velocity\n",
    "        velocity=np.add(first_term,second_term)\n",
    "        velocity=np.add(velocity,third_term)\n",
    "\n",
    "        #we update the position of the centroids of the particles\n",
    "        particles_list[i]=np.add(particles_list[i],velocity)\n",
    "        \n",
    "    ##print (\"global best error is: \", global_best_error)\n",
    "    ##print (\"global best position: \", global_best_position)\n",
    "    ##print (\"personal best error: \", personal_best_error)\n",
    "    ##print (\"Particle list AFTER\", particles_list)\n",
    "    ##print(\"personal best position  \",personal_best_position )\n",
    "\n",
    "    \n",
    "print(\"After \",iteration,\" iteration , the global best error is: \", global_best_error, \"best  swarn has positions: \", global_best_position)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can di the same thing for the irisi dataset. This time as we have 3 classes and 4 dimensions we set each particle to have 3 centroids based on 4 dmensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take the max and min of each columns in the iris dataset, so we can set limits to our randomly chosen centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sepal_length=max(iris_df.iloc[:,0])\n",
    "min_sepal_length=min(iris_df.iloc[:,0])\n",
    "\n",
    "max_sepal_width=max(iris_df.iloc[:,1])\n",
    "min_sepal_width=min(iris_df.iloc[:,1])\n",
    "\n",
    "max_petal_length=max(iris_df.iloc[:,2])\n",
    "min_petal_length=min(iris_df.iloc[:,2])\n",
    "\n",
    "max_petal_width=max(iris_df.iloc[:,3])\n",
    "min_petal_width=min(iris_df.iloc[:,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we initialize 10 particles with 3 centroid each and 4 dimensions to use in the iris dataset\n",
    "particles_list_iris=[]\n",
    "\n",
    "for i in range(0,10):\n",
    "    particle=[]\n",
    "    \n",
    "    for i in range(0,3):\n",
    "        centroid=[np.random.uniform(min_sepal_length,max_sepal_length,1),np.random.uniform(min_sepal_width,max_sepal_width,1),np.random.uniform(min_petal_width,max_petal_width,1),np.random.uniform(min_petal_width,max_petal_width,1)]\n",
    "        particle.append(centroid)\n",
    "    \n",
    "    particles_list_iris.append(particle)\n",
    "    \n",
    "particles_list_iris=np.asarray(particles_list_iris).reshape(10,3,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After  19  iteration , the global best error is:  0.8565857081242823 best  swarn has positions:  [[-3.32146349e+08 -9.38001868e+06  5.87334275e+07  7.11057973e+07]\n",
      " [-4.09695375e+07  1.79198894e+07 -2.72622289e+07 -1.08861167e+08]\n",
      " [ 3.61659108e+08  6.46734422e+07  1.74208476e+08 -2.97542908e+07]]\n"
     ]
    }
   ],
   "source": [
    "##we set our initial values\n",
    "\n",
    "personal_best_error=np.full((1, 10), 50).astype(float)\n",
    "personal_best_error=personal_best_error[0][:]\n",
    "global_best_position=[]\n",
    "global_best_error=100  ##we set this to 100 so we are sure that the algorithm will find a smaller value\n",
    "personal_best_position=np.zeros(120).reshape(10,3,4)\n",
    "\n",
    "alpha=1.49618\n",
    "w=0.7298\n",
    "velocity=np.zeros(12).reshape(3,4) \n",
    "\n",
    "##for each iteration\n",
    "for iteration in range(0,20):\n",
    "    \n",
    "    quantization_error=[] \n",
    "    count=0\n",
    "    \n",
    "    ##for each particles \n",
    "    for i in range(0,len(particles_list_iris)):\n",
    "\n",
    "        \n",
    "        current_particle=particles_list_iris[i]\n",
    "        cluster1=[]\n",
    "        cluster2=[]\n",
    "        cluster3=[]\n",
    "        \n",
    "       ##for each data point in the Artificial dataset\n",
    "        for a in range(0,len(iris_df)):\n",
    "            \n",
    "            \n",
    "            ##we measure the euclidean distance from each centroid\n",
    "            dist1=scipy.spatial.distance.euclidean(np.asarray(iris_df.loc[a]), current_particle[0])\n",
    "            dist2=scipy.spatial.distance.euclidean(np.asarray(iris_df.loc[a]), current_particle[1])\n",
    "            dist3=scipy.spatial.distance.euclidean(np.asarray(iris_df.loc[a]), current_particle[2])\n",
    "            \n",
    "            \n",
    "            ##we assign each datapoint to a cluster based on the closest centroid\n",
    "            if dist1<dist2 and dist1<dist3:\n",
    "                cluster1.append(iris_df.loc[a])\n",
    "\n",
    "\n",
    "            elif dist2<dist3 and dist2<dist3:\n",
    "                cluster2.append(iris_df.loc[a])\n",
    "                \n",
    "            elif dist3<dist1 and dist3<dist1:\n",
    "                cluster3.append(iris_df.loc[a])\n",
    "\n",
    "\n",
    "        \n",
    "        ##Once we have our cluster we compute the quantization error and we store it\n",
    "        ## we will have a list with all our quantization error\n",
    "        quantization_error.append(quantization_error3(cluster1,cluster2,cluster3,current_particle[0],current_particle[1],current_particle[2]))\n",
    "\n",
    "   \n",
    "    \n",
    "    ##Then for each quantization error in the list\n",
    "    for z in range(0,len(quantization_error)):\n",
    "        count=count+1    \n",
    "        \n",
    "        ##if the quantization error of the current iteration is smaller than the personal best , we store it as new personal best\n",
    "        if quantization_error[z]<personal_best_error[z]:\n",
    "            personal_best_error[z]=quantization_error[z]\n",
    "            ##and we store the corresponding particle as new best position\n",
    "            personal_best_position[z]=particles_list_iris[z]\n",
    "\n",
    "        ##we also update the best global error if we find a new best \n",
    "        if personal_best_error[z]<global_best_error:\n",
    "            global_best_error=personal_best_error[z]\n",
    "            global_best_position=particles_list_iris[z]\n",
    "            \n",
    "    ###print (\"Particle list BEFORE, \", particles_list)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ## then for each particle we update the position of the centroids based on the formula\n",
    "    \n",
    "    for i in range(0,len(particles_list_iris)):\n",
    "        \n",
    "        ##I've disassembled the formula for clarity.\n",
    "\n",
    "        \n",
    "\n",
    "        ##we set the random r\n",
    "        r1=np.random.uniform(0,1,1)\n",
    "        r2=np.random.uniform(0,1,1)\n",
    "\n",
    "        ##we compute the first term of the formula\n",
    "        first_term=np.multiply(w,velocity)\n",
    "        \n",
    "        ##we compute the two multiplication of the alpha and the r\n",
    "        alphaR1=alpha*r1\n",
    "        alphaR2=alpha*r2\n",
    "        \n",
    "        #we copmute the 2 subtractions\n",
    "        first_subtraction=np.subtract(personal_best_position[i],particles_list_iris[i])\n",
    "        second_subtraction=np.subtract(global_best_position,particles_list_iris[i])\n",
    "\n",
    "        #we compute the second term\n",
    "        second_term=np.multiply(alphaR1,first_subtraction)\n",
    "        \n",
    "        #we compute the second term\n",
    "        third_term=np.multiply(alphaR2,second_subtraction)\n",
    "        \n",
    "        ##we compute the velocity\n",
    "        velocity=np.add(first_term,second_term)\n",
    "        velocity=np.add(velocity,third_term)\n",
    "\n",
    "        #we update the position of the centroids of the particles\n",
    "        particles_list_iris[i]=np.add(particles_list_iris[i],velocity)\n",
    "        \n",
    "    ##print (\"global best error is: \", global_best_error)\n",
    "    ##print (\"global best position: \", global_best_position)\n",
    "    ##print (\"personal best error: \", personal_best_error)\n",
    "    ##print (\"Particle list AFTER\", particles_list)\n",
    "    ##print(\"personal best position  \",personal_best_position )\n",
    "\n",
    "    \n",
    "print(\"After \",iteration,\" iteration , the global best error is: \", global_best_error, \"best  swarn has positions: \", global_best_position)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we compare the results with the results described in the paper, we can notice that our PSO and KMEANS algorithm performs somehow similarly to the results of the paper. In fact we have:\n",
    "\n",
    "- Quantization error for the KMEans algorithm for the articifial dataset:   0.5778295493707196\n",
    "- Quantization error for the KMEans algorithm for the iris dataset:   0.6473743892616001\n",
    "\n",
    "- Quantization error for the PSO algorithm for the articifial dataset:  0.4570523934830559 \n",
    "- Quantization error for the PSO algorithm for the articifial dataset::   0.8999702242159057 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4\n",
    "\n",
    "The figure shows an example from the ACO book by Dorigo and Stuetzle. What results do you expect for an ant colony algorithm that does not use tabu lists (except for inhibition of immediate return to the previous node)?\n",
    "\n",
    "<img src=\"ACO.png\" alt=\"Drawing\" style=\"width: 30%;\"/>\n",
    "\n",
    "> ACO is inspired from the foraging behaviour of ants and imitates how they find the closest food sources to their nests. They communicate with pheromones they leave behind while moving. Likewise, they tend to choose paths marked by strong pheromone concentrations. When an ant finds a food source, it leaves some amount of pheromone that may depend on the quantity and quality of the food during the return trip. Thus, quality food sources closest to the nest will have the path with more pheromone. \n",
    "However in this case without a tabu list, it is unavoidable that ants make cycles. At the bottom of the picture above, there are a lot of connected paths and this will cause a lot of pheromone accumulation and will result with most likely loopy paths. For such situations converging the minumum path is not trivial.\n",
    "To find the optimum solution an ant has to make number of correct desicions, otherwise ant generates suboptimal solutions.\n",
    "There is a trade off between use of an easy but suboptimal path and searching the optimal path. \n",
    "In this case, easy path would be that the ants take the route through the upper nodes. There are two optimal paths which are in the lower part of the graph and harder to find.\n",
    "We can avoid from cycles with high pheromone evaporation. However, this may end up with not generating optimum solutions. Since this problem is relatively simple and can be solved with a large number of ants. After a lot of iterations algorithm find the fastest route in the densely connected part of graph which is down-up-right-down-up or down-down-rigt-up-up.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5\n",
    "\n",
    "Assume that ants are allowed to lay pheromone on a path at every time step, so that the pheromone update rule is applied at each time step. Come up with a combination local/global updating scheme that encourages exploration and exploitation - consider which parameters influence this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Ants probabilisticaly choose the next node based on the amount of pheromone while it is searching for food and exploring the space.\n",
    "For each ant we can keep a limited memory which is local pheromone. This memory allows them to record their trip to their destination. When they reach their destination they can construct a way back to home. If they've passed over the same node twice they can eliminate the path in between those repeated nodes. Then they will retrace the steps they constructed based on the memory(local pheromone). On the way back to the nest ant will ignore the pheromone trails, follow the path that is costructed.\n",
    "When they are going back to the nest they will be leaving pheromones to the ground(global pheromones). They will not apply to the global pheromones on the food searching step because that can cause loops.\n",
    "After each round the new pheromones are added to the old ones. For better exploitation, good ants should influence the new pheromones more than others.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 6\n",
    "\n",
    "The goal of this exercise is to practice with an implementation of ACO, so please do not use other (possibly better) methods for solving this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Code an ACO to solve Sudoku’s. First, you need to think how to represent Sudoku: there are three conditions an optimal solution must fulfill, each line must contain all integers {1, 2, 3, 4, 5, 6, 7, 8, 9} once and only once, the same applies to columns and 3x3 subgrids. You can fix one of them so that you will prevent violations. The ACO will have 9x9x9 pheromone matrix, where you update pheromones for the values appearing in the best solutions inversely proportionally to the fitness value. You have to consider also how the old pheromones will fade.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import math\n",
    "import sys\n",
    "import random\n",
    "\n",
    "# inputfile = \"sudoku/s10b.txt\"\n",
    "iteration_number = 10 \n",
    "r = 0.05\n",
    "ants = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ACO_Algorithm(a, t, ants):\n",
    "    sudoku_problem = a.copy()   \n",
    "    bestfitness = 0             #initialize the best fitness\n",
    "    bestsolution = []           # initialize the best solution\n",
    "    for i in range (1, ants+1):             # create all ants\n",
    "        fitness, solution = ant(sudoku_problem,t)  # retrieve the solution and fitness from the ant given the problem and pheramone matrix\n",
    "        if fitness > bestfitness:           # compare fitness to best fitness so far \n",
    "            bestfitness = fitness           # save new fitness value if greater than previous\n",
    "            bestsolution = solution.copy()  # copy the solutions\n",
    "            if np.count_nonzero(solution) == 81:   # if there is no zero left then the answer is found \n",
    "                break                       \n",
    "    return bestfitness, bestsolution        # return best solution and fitness for one iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_solution(ant_solution, digits, row_matrix, digit_submatrix, column_matrix):\n",
    "    # for each position if there are obvious digits to fill, Fill the digit with that number\n",
    "    for (i,j), p in np.ndenumerate(digits):\n",
    "        # calculate index z for digit_submatrix\n",
    "        if i < 3: \n",
    "            z = int((j/3)+1)\n",
    "        elif 2 < i < 6: \n",
    "            z = int((j/3)+1)+3\n",
    "        elif 5 < i: \n",
    "            z = int((j/3)+1)+6\n",
    "\n",
    "        # calculate the list of digits that are taken for this position\n",
    "        digit_list = list(row_matrix[i] + column_matrix[j] + digit_submatrix[z-1])\n",
    "\n",
    "        # count how many are available to fill\n",
    "        digit_possible = digit_list.count(0) \n",
    "\n",
    "        # if there are possible digits, save the number in the digits matrix\n",
    "        if ant_solution[i][j] > 0: \n",
    "            digit_possible = 0\n",
    "        digits[i][j]=digit_possible\n",
    "\n",
    "        # if only one digit is possible, find it and put it in the solution\n",
    "        if digit_possible == 1 and ant_solution[i][j] == 0:\n",
    "            ant_solution[i][j]=digit_list.index(0)+1\n",
    "            digits[i][j]=0\n",
    "            break\n",
    "\n",
    "    return ant_solution, digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_memory(ant_solution, row_matrix, digit_submatrix, column_matrix):\n",
    "    # Create a memory of taken numbers in submatrixes, rows and columns\n",
    "    count = 0\n",
    "    for x in [0,3,6]:\n",
    "        for y in [0,3,6]:\n",
    "            for (i2,j2),sd in np.ndenumerate(ant_solution[x:x+3,y:y+3]):\n",
    "                if sd > 0:\n",
    "                    digit_submatrix[count][sd-1]=1\n",
    "            count += 1\n",
    "    # for each row and column, check which numbers are already taken\n",
    "    for (i,j), digit in np.ndenumerate(ant_solution):\n",
    "        if digit > 0:\n",
    "            row_matrix[i][digit-1] = 1\n",
    "            column_matrix[j][digit-1] = 1\n",
    "\n",
    "    return row_matrix, digit_submatrix, column_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ant(d,t):\n",
    "    #initialize the antas solution set \n",
    "    ant_solution = d.copy()   \n",
    "\n",
    "    current_fitness = 0\n",
    "    row_matrix = np.zeros((9,9)) # initialize digit matrix in each row\n",
    "    column_matrix = np.zeros((9,9)) # initialize digit matrix in each column\n",
    "    digit_submatrix = np.zeros((9,9), dtype=np.int) # initialize the matrix for possible digits in each 3x3 submatrix\n",
    "    digits = np.zeros((9,9)) # initialize the possible digits matrix\n",
    "    while (True):\n",
    "        \n",
    "        row_matrix, digit_submatrix, column_matrix = update_memory(ant_solution, row_matrix, digit_submatrix, column_matrix)\n",
    "        ant_solution, digits = update_solution(ant_solution, digits, row_matrix, digit_submatrix, column_matrix)\n",
    "        \n",
    "        # if there are no zeros in ant_solution answer is found, kill the ant and return the solution\n",
    "        if np.count_nonzero(ant_solution) == 81: \n",
    "            return current_fitness, ant_solution\n",
    "        \n",
    "        #if no more steps are possible based on analytics\n",
    "        if(np.count_nonzero(ant_solution)==current_fitness):\n",
    "            w = np.zeros((9,9,9)) # initialize weight matrix\n",
    "            wsum = 0 # sum of weights\n",
    "            # for each weight, fill in the actual weight\n",
    "            for (i,j,k), v in np.ndenumerate(w):\n",
    "                if i < 3: \n",
    "                    z = int((j/3)+1)\n",
    "                elif 2 < i < 6: \n",
    "                    z = int((j/3)+1)+3\n",
    "                elif 5 < i: \n",
    "                    z = int((j/3)+1)+6\n",
    "                \n",
    "                if digits[i][j] > 0:\n",
    "                    # weight equation: pheromone * 10-number of possible places for the digit * 10-number of possible digits for the position\n",
    "                    w[i][j][k] = t[i][j][k]*(10-list(digit_submatrix[z-1]).count(0))*(10-digits[i][j])\n",
    "                    # sum up the weights\n",
    "                    wsum += w[i][j][k]\n",
    "            \n",
    "            # in case there are no more options left return\n",
    "            if wsum == 0: \n",
    "                return current_fitness, ant_solution\n",
    "            \n",
    "            # calculate a random value between 0 and the sum of weights and normalize it\n",
    "            next_selection = random.randint(0,int(wsum))/wsum\n",
    "            # iterate through possible digits until the random value has been reached, effectively a weighted roulette wheel\n",
    "            psum = 0\n",
    "            for (i,j,k), v in np.ndenumerate(w):\n",
    "                psum += w[i][j][k]/wsum\n",
    "                # if this digit is the chosen one.. \n",
    "                if psum > next_selection:\n",
    "                    # generate z index for 3x3\n",
    "                    if i < 3: \n",
    "                        z = int((j/3)+1)\n",
    "                    if 2 < i < 6: \n",
    "                        z = int((j/3)+1)+3\n",
    "                    if 5 < i: \n",
    "                        z = int((j/3)+1)+6\n",
    "                    # check for validity and add to solution\n",
    "                    if row_matrix[i][k] == 0 and column_matrix[j][k] == 0 and digit_submatrix[z-1][k] == 0:\n",
    "                        ant_solution[i][j] = k+1\n",
    "                    else:\n",
    "                        return current_fitness, ant_solution\n",
    "                    \n",
    "                    break\n",
    "        else:\n",
    "            current_fitness = np.count_nonzero(ant_solution)          \n",
    "    return ant_solution, current_fitness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Consider the benchmark Sudoku problems are available at http://lipas.uwasa.fi/~timan/sudoku/. Run your ACO on the following benchmark instances s10a.txt, s10b.txt, s11a.txt, s11b.txt. Report and discuss the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "for the problem at sudoku/s10a.txt\n",
      "\n",
      "[[9 7 2 8 6 3 5 4 1]\n",
      " [6 1 8 7 4 5 9 2 3]\n",
      " [4 5 3 2 9 1 6 8 7]\n",
      " [5 4 9 1 2 8 7 3 6]\n",
      " [8 2 1 6 3 7 4 5 9]\n",
      " [7 3 6 4 5 9 2 1 8]\n",
      " [2 9 5 3 8 6 1 7 4]\n",
      " [1 8 4 9 7 2 3 6 5]\n",
      " [3 6 7 5 1 4 8 9 2]]\n",
      "The problem is solved at iteration 1\n",
      "\n",
      "for the problem at sudoku/s10b.txt\n",
      "\n",
      "[[8 5 2 7 1 6 9 4 3]\n",
      " [1 9 7 8 4 3 6 5 2]\n",
      " [4 6 3 9 2 5 1 8 7]\n",
      " [2 7 8 6 3 4 5 9 1]\n",
      " [6 4 5 1 7 9 3 2 8]\n",
      " [9 3 1 5 8 2 4 7 6]\n",
      " [7 8 6 4 9 1 2 3 5]\n",
      " [3 1 4 2 5 8 7 6 9]\n",
      " [5 2 9 3 6 7 8 1 4]]\n",
      "The problem is solved at iteration 1\n",
      "\n",
      "for the problem at sudoku/s11a.txt\n",
      "\n",
      "[[3 0 5 0 7 0 2 0 0]\n",
      " [2 7 9 0 5 0 1 8 0]\n",
      " [8 0 1 0 2 0 5 0 7]\n",
      " [1 9 7 3 4 6 8 5 2]\n",
      " [4 5 2 7 1 8 3 9 6]\n",
      " [6 8 3 5 9 2 7 0 0]\n",
      " [7 0 8 0 6 0 9 0 5]\n",
      " [5 1 6 9 3 7 4 2 8]\n",
      " [9 0 4 0 8 0 6 0 0]]\n",
      "The problem is solved at iteration 9\n",
      "\n",
      "for the problem at sudoku/s11b.txt\n",
      "\n",
      "[[1 0 0 7 6 9 2 8 5]\n",
      " [7 5 8 2 1 3 6 4 9]\n",
      " [9 6 2 4 8 5 1 7 3]\n",
      " [5 0 0 8 0 0 0 1 6]\n",
      " [2 0 0 0 0 0 0 0 4]\n",
      " [8 9 0 0 0 6 0 0 7]\n",
      " [6 1 9 3 7 8 4 5 2]\n",
      " [3 2 0 9 5 4 7 6 8]\n",
      " [4 8 0 6 2 0 0 0 1]]\n",
      "The problem is solved at iteration 9\n"
     ]
    }
   ],
   "source": [
    "#open inputfile and take sudoku into problem_matrix\n",
    "inputfiles = [\"sudoku/s10a.txt\", \"sudoku/s10b.txt\", \"sudoku/s11a.txt\",\"sudoku/s11b.txt\"]\n",
    "for inputfile in inputfiles:\n",
    "    print(\"\\nfor the problem at \" + inputfile + \"\\n\")\n",
    "    # Create the matrixes;\n",
    "    problem_matrix = np.zeros((9,9), dtype=np.int)  # initialize problem matrix    \n",
    "    global_pheromone= np.ones((9,9,9))*1000           # initialize pheromene matrix\n",
    "    globalbestfitness = 0                           # initialize global best fitness\n",
    "    globalbestsolution = []                         # initialize global best solution matrix\n",
    "\n",
    "    with open(inputfile, \"r\") as f:\n",
    "        for i,line in enumerate(f):\n",
    "            digitlist = line.split()\n",
    "            if not digitlist == []:\n",
    "                for j,digitstr in enumerate(digitlist):\n",
    "                    problem_matrix[i][j] = int(digitstr)\n",
    "\n",
    "\n",
    "    for iteration in range (1,iteration_number):\n",
    "        # the best solution & fitness for iteration\n",
    "        bestfitnessofiteration, bestsolutionofiteration = ACO_Algorithm(problem_matrix,global_pheromone,ants)\n",
    "        # if it is better than the global best, copy it\n",
    "        if(bestfitnessofiteration>globalbestfitness):\n",
    "            globalbestfitness = bestfitnessofiteration\n",
    "            globalbestsolution = bestsolutionofiteration.copy()\n",
    "        # if it is solved, stop iteration\n",
    "        if(np.count_nonzero(globalbestsolution) == 81):\n",
    "            break\n",
    "        # equation for new pheromone to be added\n",
    "        add_pheromone = (bestfitnessofiteration/81)\n",
    "        # pheromone decay according to rate r\n",
    "        for (i,j,k), phe in np.ndenumerate(global_pheromone):\n",
    "            global_pheromone[i][j][k] = (1-r)*global_pheromone[i][j][k]\n",
    "        # add new pheromone for best solution set\n",
    "        for (i,j), k in np.ndenumerate(bestsolutionofiteration):\n",
    "            if int(k) > 0:\n",
    "                # pheromone update equation, the constant can be modified to change behavior\n",
    "                 global_pheromone[i][j][int(k-1)] += 1000*add_pheromone\n",
    "    print(globalbestsolution)\n",
    "    print('The problem is solved at iteration ' + str(iteration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "s10a and s10b are solved in one iteration because they can be solved only using analytical methods. However for s11a and s11b and iteration number to solve them is between 1 and 10.\n",
    "We choose 0.05 evaporation rate because we want pheromones to decay quickly but not quickly. Also, we use 5 ants "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

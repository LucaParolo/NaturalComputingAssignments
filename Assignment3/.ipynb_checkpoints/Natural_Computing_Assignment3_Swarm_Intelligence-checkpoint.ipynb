{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Computing Assignment 3 \n",
    "Simge Ekiz(s4706757), Luca Parola(s1009497), Katrin Bujari(s1005213)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Consider an illustrative example of a PSO system composed of three particles. Consider the following update rule for each particle i and dimension d:\n",
    "\\begin{equation}\n",
    "\t\tv(i;d) = wv(i;d) + r_1({x}^*(i;d) - x(i;d)) + r_2({x}^*(d) - x(i;d)) \n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "To facilitate calculation, we will ignore the fact that $r_1$ and $r_2$ are random numbers and fix them to $0.5$ for this exercise. The space of solutions is the two dimensional real valued space and the current state of the swarm is as follows\n",
    "\n",
    "-  Position of particles: $x_1 = (5,5); x_2 = (8,3); x_3 = (6,7)$\n",
    "-  Individual best positions: $x^*_1 = (5,5); x^*_2 = (7,3); x^*_3 = (5,6)$\n",
    "-  Social best position: $x^* = (5,5)$\n",
    "-  Velocities: $v_1 = (2,2); v_2 = (3,3); v_3 = (4,4)$\n",
    "\n",
    "\n",
    "a-) What would be the next position of each particle after one iteration of the PSO algorithm with w = 2?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding-left: 20px;\">\n",
    "$w = 2$\n",
    "$r_1 = 0.5$\n",
    "$r_2 = 0.5$\n",
    "<br><br>\n",
    "<b> According to formula; </b><br>\n",
    "  <span style=\"padding-left: 15px;\" >  $v(i;d) = wv(i;d) + r_1(x^∗(i;d) - x(i;d)) + r2(x^∗(d) - x(i;d))$\n",
    "  </span>\n",
    "<br><br>\n",
    "<b> Velocity of particle 1; </b><br> \n",
    "<span style=\"padding-left: 15px;\" >\n",
    "    $v(1;xaxis) = 2*2 + 0.5*(5-5) + 0.5*(5-5) = 4$ <br>\n",
    "</span>\n",
    "<span style=\"padding-left: 15px;\" >\n",
    "    $v(1;yaxis) = 2*2 + 0.5*(5-5) + 0.5*(5-5) = 4$ <br> \n",
    "</span>\n",
    "<span style=\"padding-left: 15px;\" >\n",
    "    ${v}_1^2 = (4, 4)$ \n",
    "</span>\n",
    "<br><br>\n",
    "<b> Velocity of particle 2;</b> <br>\n",
    "<span style=\"padding-left: 15px;\" >\n",
    "    $v(2;xaxis) = 2*3 + 0.5*(7-8) + 0.5*(5-8) = 4$ <br>\n",
    "</span>\n",
    "<span style=\"padding-left: 15px;\" >\n",
    "    $v(2;yaxis) = 2*3 + 0.5*(3-3) + 0.5*(5-3) = 7$ <br>\n",
    "</span>\n",
    "<span style=\"padding-left: 15px;\" >\n",
    "    $v_2^2 = (4, 7)$ \n",
    "</span>\n",
    "<br><br>\n",
    "<b> Velocity of particle 3; </b><br>\n",
    "<span style=\"padding-left: 15px;\" >\n",
    "    $v(3;xaxis) = 2*4 + 0.5*(5-6) + 0.5*(5-6) = 7$ <br>\n",
    "</span>\n",
    "<span style=\"padding-left: 15px;\" >\n",
    "    $v(3;yaxis) = 2*4 + 0.5*(6-7) + 0.5*(5-7) = 6.5$ <br>\n",
    "</span>\n",
    "<span style=\"padding-left: 15px;\" >\n",
    "    $v_3^2 = (7, 6.5)$ \n",
    "</span>\n",
    "<br><br>\n",
    "<b> Updating positions : </b> <br>\n",
    "<span style=\"padding-left: 15px;\" >\n",
    "    $x_i^{t+1} = x_i^{t} + v_i^{t+1}$\n",
    "</span><br><br>\n",
    "<b> Position of particle 1; </b> <br>\n",
    "<span style=\"padding-left: 15px;\" >\n",
    "$x_1^2 = (5, 5) + v_1^2 = (5, 5) + (4, 4) = (9, 9) $\n",
    "</span>\n",
    "<br><br>\n",
    "<b> Position of particle 2; </b><br>\n",
    "<span style=\"padding-left: 15px; \" >\n",
    "$x_2^2 = (8,3) + v_2^2 = (8, 3) + (4, 7) = (12, 10) $ \n",
    "</span> <br><br>\n",
    "<b>Position of particle 3; </b> <br>\n",
    "<span style=\"padding-left: 15px;\" >\n",
    "$x_3^2 = (6,7) + v_3^2 = (6, 7) + (7, 6.5) = (13, 13.5) $ \n",
    "</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b-) And using w = 0.1?\n",
    "\n",
    "<div style=\"padding: 20px;\">\n",
    "$w=0.1$\n",
    "$r_1 = 0.5$\n",
    "$r_2 = 0.5$\n",
    "<br><br>\n",
    "<b>Velocity of particle 1;</b> <br> \n",
    "<span style=\"padding-left: 15px;\" >\n",
    "    $v(1;xaxis) = 0.1*2 + 0.5*(5-5) + 0.5*(5-5) = 0.2$ <br>\n",
    "</span>\n",
    "<span style=\"padding-left: 15px;\" >\n",
    "    $v(1;yaxis) = 0.1*2 + 0.5*(5-5) + 0.5*(5-5) = 0.2$ <br> \n",
    "</span>\n",
    "<br>\n",
    "<b>Velocity of particle 2;</b> <br>\n",
    "<span style=\"padding-left: 15px;\" >\n",
    "    $v(2;xaxis) = 0.1*3 + 0.5*(7-8) + 0.5*(5-8) = -1.7$ <br>\n",
    "</span>\n",
    "<span style=\"padding-left: 15px;\" >\n",
    "    $v(2;yaxis) = 0.1*3 + 0.5*(3-3) + 0.5*(5-3) = 1.3$ <br>\n",
    "</span>\n",
    "<br>\n",
    "<b>Velocity of particle 3;</b> <br>\n",
    "<span style=\"padding-left: 15px;\" >\n",
    "    $v(3;xaxis) = 0.1*4 + 0.5*(5-6) + 0.5*(5-6) = -0.6$ <br>\n",
    "</span>\n",
    "<span style=\"padding-left: 15px;\" >\n",
    "    $v(3;yaxis) = 0.1*4 + 0.5*(6-7) + 0.5*(5-7) = -1.1$ <br>\n",
    "</span>\n",
    "<br><br>\n",
    "\n",
    "<b>Position of particle 1;</b> <br>\n",
    "<span style=\"padding-left: 15px;\" >\n",
    "$x_1^2 = (5, 5) + (0.2, 0.2) = (5.2, 5.2) $\n",
    "</span>\n",
    "<br><br>\n",
    "<b>Position of particle 2;</b> <br>\n",
    "<span style=\"padding-left: 15px; \" >\n",
    "$x_2^2 = (8, 3) + (-1.7, 1.3) = (6.3, 4.3) $ \n",
    "</span> <br><br>\n",
    "<b>Position of particle 3; </b> <br>\n",
    "<span style=\"padding-left: 15px;\" >\n",
    "$x_3^2 = (6, 7) + (-0.6, -1.1) = (5.4, 5.9) $ \n",
    "</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c-) Explain what is the effect of the parameter w.\n",
    "> In PSO, for each particle, velocity is calculated according to its previous velocity, the particle location which has the best fitness so far and any particles location which has the best fitness so far. W carries a factor that how much the previous velocity contribute to the next velocity. small w facilitates local search, it exploits the current position as much as it can. Large w facilitates global search, it aims to explore solution space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d-) Give an advantage and a disadvantage of a high value of w.\n",
    "> An advantage of high value w is that it allows us to explore more. Particles move faster and more solution space could be searced and this increase the possibility of finding a better solution.\n",
    "A disadvantage of of high value w is that;\n",
    "it does less expoitation and gets less sensitive to new information in the influence from the current particle. Thus, it might skip over an optimum.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Consider a particle swarm consisting of a single member. How would it perform in a\n",
    "trivial task such as the minimization of $f(x) = x^2$ when $w < 1$?\n",
    "\n",
    "> Given that w<1 and $f(x) = x^2$ It would find the optimum solution. Finding a solution depends on the objective function since, f(x) establishes a convex solution space which has less complexity of the search space. For instance, If the particle starts with the velocity which drawns the particle away from the optimum, personal best effect the particle and after some time particle will be moving to optimum point. When velocity is in the same direction with the optimum, personal best loose its influence. Then, velocity start to decrease. If the particle reaches to optimum before the velocity is zero, algorithm finds the optimum point. Particle will find the optimum for this trivial problem, because when w < 1, particles would be moving around the optimum. Then, after some steps velocity would be zero at the optimum.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "Implement the PSO algorithm for clustering described in “Van der Merwe, D. W., and Andries Petrus Engelbrecht. ”Data clustering using particle swarm optimization.” Evolutionary Computation, 2003. CEC’03. The 2003 Congress on. Vol. 1. IEEE, 2003.” (see also swarm intelligence slides). Implement the k-means clustering.\n",
    "Apply and compare the performance of the two algorithms in terms of quantization error on Artificial dataset 1 and Iris dataset (the latter available at UCI ML repository, see https://archive.ics.uci.edu/ml/datasets/iris). In both algorithms, use the true number of clusters as value of the parameter for setting the number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build the Artificial dataset defined as the equation 9 in Van der Merwe, D. W., and\n",
    "Andries Petrus Engelbrecht. \"Data clustering using particle swarm optimization.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z1=np.random.uniform(-1,1,400)\n",
    "z2=np.random.uniform(-1,1,400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "artificial_df=pd.DataFrame({\"Z1\" : z1,\"Z2\" : z2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kmeans1 = KMeans(n_clusters=2, random_state=0).fit(artificial_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We predict the class of each datapoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction_artificial_df=kmeans1.predict(artificial_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compute the quantization error for the artificial dataset\n",
    "\n",
    "### quantization error\n",
    "\n",
    "$$J_{e}\\,=\\,{{\\sum_{j=1}^{N_{c}}[\\sum_{\\forall {\\bf z}_{{\\rm p}}\\in C_{ij}} d({\\bf z}_{p},{\\bf m}_{j})/\\vert C_{ij}\\vert]}\\over{N_{c}}}$$\n",
    "\n",
    "\n",
    "$$d ( {{\\bf z}_{p}, m_{j}})\\,=\\,\\sqrt{\\sum_{k=1}^{N_{j}}(z_{pk}-m_{jk})^{2}}$$\n",
    "\n",
    "\n",
    "\n",
    "$N_{c}$ denotes the number of cluster centroids (as provided by the user), i.e. the number of dusters to be formed\n",
    "\n",
    "${\\bf z}_{p}$ denotes the p-th data vector\n",
    "\n",
    "$C_{j}$ is the subset of data vectors that form cluster j.\n",
    "\n",
    "$m_{j}$ is the centroid \n",
    "\n",
    "$d_{{\\bf z}_{p}, m_{j}}$ is the Euclidean distance \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##we take the centroids:\n",
    "centroid1_A_df=kmeans1.cluster_centers_[0]\n",
    "centroid2_A_df=kmeans1.cluster_centers_[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## we assign each data point to the predicted cluster \n",
    "cluster1_A=[]\n",
    "cluster2_A=[]\n",
    "for i in range(0,len(artificial_df)):\n",
    "    if prediction_artificial_df[i]==0:\n",
    "        \n",
    "        cluster1_A.append(artificial_df.loc[i])\n",
    "        \n",
    "    if prediction_artificial_df[i]==1:\n",
    "        cluster2_A.append(artificial_df.loc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def quantization_error2(cluster1,cluster2,centroid1,centroid2):\n",
    "            \n",
    "    distance1=0\n",
    "    distance2=0\n",
    "\n",
    "\n",
    "    for i in range(0,len(cluster1)):\n",
    "\n",
    "        dist1=scipy.spatial.distance.euclidean(np.asarray(cluster1[i]), centroid1)\n",
    "        distance1=distance1+dist1\n",
    "    \n",
    "    if len(cluster1)>0:\n",
    "        distance1=distance1/len(cluster1)\n",
    "    else:\n",
    "        distance1=0\n",
    "\n",
    "    for i in range(0,len(cluster2)):\n",
    "\n",
    "        dist2=scipy.spatial.distance.euclidean(np.asarray(cluster2[i]), centroid2)\n",
    "        distance2=distance2+dist2\n",
    "    \n",
    "    if len(cluster2)>0:\n",
    "        distance2=distance2/len(cluster2)\n",
    "    else:\n",
    "        distance2=0\n",
    "\n",
    "\n",
    "    QE=(distance1+distance2)/2\n",
    "    ##print(\"Quantization error: \",QE)\n",
    "    return QE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quantization error for the KMEANS algorithm for the artificial dataset is   0.5778295493707196\n"
     ]
    }
   ],
   "source": [
    "print (\"The quantization error for the KMEANS algorithm for the artificial dataset is  \", quantization_error2(cluster1_A,cluster2_A,centroid1_A_df,centroid2_A_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can do the same with the iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "y=iris.target\n",
    "iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kmeans2 = KMeans(n_clusters=3, random_state=0).fit(iris_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction_iris_df=kmeans2.predict(iris_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##we take the centroids:\n",
    "centroid1_iris=kmeans2.cluster_centers_[0]\n",
    "centroid2_iris=kmeans2.cluster_centers_[1]\n",
    "centroid3_iris=kmeans2.cluster_centers_[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## we assign each data point to the predicted cluster \n",
    "cluster1_iris=[]\n",
    "cluster2_iris=[]\n",
    "cluster3_iris=[]\n",
    "for i in range(0,len(iris_df)):\n",
    "    if prediction_iris_df[i]==0:\n",
    "        cluster1_iris.append(iris_df.loc[i])\n",
    "    if prediction_iris_df[i]==1:\n",
    "        cluster2_iris.append(iris_df.loc[i])\n",
    "    if prediction_iris_df[i]==2:\n",
    "        cluster3_iris.append(iris_df.loc[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our qunatization error function for 3 cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def quantization_error3(cluster1,cluster2,cluster3,centroid1,centroid2,centroid3):\n",
    "            \n",
    "    distance1=0\n",
    "    distance2=0\n",
    "    distance3=0\n",
    "    count=0\n",
    "    for i in range(0,len(cluster1)):\n",
    "\n",
    "        dist1=scipy.spatial.distance.euclidean(np.asarray(cluster1[i]), centroid1)\n",
    "        distance1=distance1+dist1\n",
    "    \n",
    "    if len(cluster1)>0:\n",
    "        distance1=distance1/len(cluster1)\n",
    "    else:\n",
    "        distance1=0\n",
    "    \n",
    "   \n",
    "    \n",
    "\n",
    "    for i in range(0,len(cluster2)):\n",
    "\n",
    "        dist2=scipy.spatial.distance.euclidean(np.asarray(cluster2[i]), centroid2)\n",
    "        distance2=distance2+dist2\n",
    "    \n",
    "    if len(cluster2)>0:\n",
    "        distance2=distance2/len(cluster2)\n",
    "    else:\n",
    "        distance2=0\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "    for i in range(0,len(cluster3)):\n",
    "\n",
    "        dist3=scipy.spatial.distance.euclidean(np.asarray(cluster3[i]), centroid3)\n",
    "        distance3=distance3+dist3\n",
    "    \n",
    "    if len(cluster3)>0:\n",
    "        distance3=distance3/len(cluster3)\n",
    "    else:\n",
    "        distance3=0\n",
    "\n",
    "\n",
    "    \n",
    "    QE=(distance1+distance2+distance3)/3\n",
    "    return QE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quantization error for the KMEANS algorithm for the iris dataset is   0.6473743892616001\n"
     ]
    }
   ],
   "source": [
    "print (\"The quantization error for the KMEANS algorithm for the iris dataset is  \", quantization_error3(cluster1_iris,cluster2_iris,cluster3_iris,centroid1_iris,centroid2_iris,centroid3_iris))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can build the PSO algorithm and use it with the Artificial DATaset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#we initialize 10 particles with 2 centroid each to use in the Artificial dataset\n",
    "particles_list=[]\n",
    "for i in range(0,10):\n",
    "    particles=np.random.uniform(-1,1,4).reshape(2,2)\n",
    "    particles_list.append(particles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After  9  iteration , the global best error is:  0.4570523934830559 best  swarn has positions:  [[ 0.53278471 -0.25095812]\n",
      " [ 1.87142303 -1.13583857]]\n"
     ]
    }
   ],
   "source": [
    "##we set our initial values\n",
    "\n",
    "personal_best_error=np.ones(10)\n",
    "global_best_position=[]\n",
    "global_best_error=1\n",
    "personal_best_position=np.zeros(40).reshape(10,2,2)\n",
    "\n",
    "alpha=1.49618\n",
    "w=0.7298\n",
    "velocity=np.zeros(4).reshape(2,2) \n",
    "\n",
    "##for each iteration\n",
    "for iteration in range(0,10):\n",
    "    \n",
    "    quantization_error=[] \n",
    "    count=0\n",
    "    \n",
    "    ##for each particles \n",
    "    for i in range(0,len(particles_list)):\n",
    "\n",
    "        \n",
    "        current_particle=particles_list[i]\n",
    "        cluster1=[]\n",
    "        cluster2=[]\n",
    "        \n",
    "       ##for each data point in the Artificial dataset\n",
    "        for a in range(0,len(artificial_df)):\n",
    "            \n",
    "            \n",
    "            ##we measure the euclidean distance from each centroid\n",
    "            dist1=scipy.spatial.distance.euclidean(np.asarray(artificial_df.loc[a]), current_particle[0])\n",
    "            dist2=scipy.spatial.distance.euclidean(np.asarray(artificial_df.loc[a]), current_particle[1])\n",
    "\n",
    "            ##we assign each datapoint to a cluster based on the closest centroid\n",
    "            if dist1<dist2:\n",
    "                cluster1.append(artificial_df.loc[a])\n",
    "\n",
    "\n",
    "            elif dist1>dist2:\n",
    "                cluster2.append(artificial_df.loc[a])\n",
    "\n",
    "\n",
    "        \n",
    "        ##Once we have our cluster we compute the quantization error and we store it\n",
    "        ## we will have a list with all our quantization error\n",
    "        quantization_error.append(quantization_error2(cluster1,cluster2,current_particle[0],current_particle[1]))\n",
    "\n",
    "   \n",
    "    \n",
    "    ##Then for each quantization error in the list\n",
    "    for z in range(0,len(quantization_error)):\n",
    "        count=count+1    \n",
    "        \n",
    "        ##if the quantization error of the current iteration is smaller than the personal best , we store it as new personal best\n",
    "        if quantization_error[z]<personal_best_error[z]:\n",
    "            personal_best_error[z]=quantization_error[z]\n",
    "            ##and we store the corresponding particle as new best position\n",
    "            personal_best_position[z]=particles_list[z]\n",
    "\n",
    "        ##we also update the best global error if we find a new best \n",
    "        if personal_best_error[z]<global_best_error:\n",
    "            global_best_error=personal_best_error[z]\n",
    "            global_best_position=particles_list[z]\n",
    "            \n",
    "    ###print (\"Particle list BEFORE, \", particles_list)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ## then for each particle we update the position of the centroids based on the formula\n",
    "    \n",
    "    for i in range(0,len(particles_list)):\n",
    "        \n",
    "        ##I've disassembled the formula for clarity.\n",
    "\n",
    "        \n",
    "\n",
    "        ##we set the random r\n",
    "        r1=np.random.uniform(0,1,1)\n",
    "        r2=np.random.uniform(0,1,1)\n",
    "\n",
    "        ##we compute the first term of the formula\n",
    "        first_term=np.multiply(w,velocity)\n",
    "        \n",
    "        ##we compute the two multiplication of the alpha and the r\n",
    "        alphaR1=alpha*r1\n",
    "        alphaR2=alpha*r2\n",
    "        \n",
    "        #we copmute the 2 subtractions\n",
    "        first_subtraction=np.subtract(personal_best_position[i],particles_list[i])\n",
    "        second_subtraction=np.subtract(global_best_position,particles_list[i])\n",
    "\n",
    "        #we compute the second term\n",
    "        second_term=np.multiply(alphaR1,first_subtraction)\n",
    "        \n",
    "        #we compute the second term\n",
    "        third_term=np.multiply(alphaR2,second_subtraction)\n",
    "        \n",
    "        ##we compute the velocity\n",
    "        velocity=np.add(first_term,second_term)\n",
    "        velocity=np.add(velocity,third_term)\n",
    "\n",
    "        #we update the position of the centroids of the particles\n",
    "        particles_list[i]=np.add(particles_list[i],velocity)\n",
    "        \n",
    "    ##print (\"global best error is: \", global_best_error)\n",
    "    ##print (\"global best position: \", global_best_position)\n",
    "    ##print (\"personal best error: \", personal_best_error)\n",
    "    ##print (\"Particle list AFTER\", particles_list)\n",
    "    ##print(\"personal best position  \",personal_best_position )\n",
    "\n",
    "    \n",
    "print(\"After \",iteration,\" iteration , the global best error is: \", global_best_error, \"best  swarn has positions: \", global_best_position)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can di the same thing for the irisi dataset. This time as we have 3 classes and 4 dimensions we set each particle to have 3 centroids based on 4 dmensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take the max and min of each columns in the iris dataset, so we can set limits to our randomly chosen centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_sepal_length=max(iris_df.iloc[:,0])\n",
    "min_sepal_length=min(iris_df.iloc[:,0])\n",
    "\n",
    "max_sepal_width=max(iris_df.iloc[:,1])\n",
    "min_sepal_width=min(iris_df.iloc[:,1])\n",
    "\n",
    "max_petal_length=max(iris_df.iloc[:,2])\n",
    "min_petal_length=min(iris_df.iloc[:,2])\n",
    "\n",
    "max_petal_width=max(iris_df.iloc[:,3])\n",
    "min_petal_width=min(iris_df.iloc[:,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#we initialize 10 particles with 3 centroid each and 4 dimensions to use in the iris dataset\n",
    "particles_list_iris=[]\n",
    "\n",
    "for i in range(0,10):\n",
    "    particle=[]\n",
    "    \n",
    "    for i in range(0,3):\n",
    "        centroid=[np.random.uniform(min_sepal_length,max_sepal_length,1),np.random.uniform(min_sepal_width,max_sepal_width,1),np.random.uniform(min_petal_width,max_petal_width,1),np.random.uniform(min_petal_width,max_petal_width,1)]\n",
    "        particle.append(centroid)\n",
    "    \n",
    "    particles_list_iris.append(particle)\n",
    "    \n",
    "particles_list_iris=np.asarray(particles_list_iris).reshape(10,3,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After  19  iteration , the global best error is:  0.8999702242159057 best  swarn has positions:  [[ 4.25079710e+07 -1.31076707e+07 -1.39633037e+08  1.84153135e+07]\n",
      " [-1.10867177e+08  6.51295081e+07  5.39108174e+07 -3.28589939e+07]\n",
      " [ 7.82919159e+07 -1.23255213e+07 -1.11076469e+08  2.35742279e+06]]\n"
     ]
    }
   ],
   "source": [
    "##we set our initial values\n",
    "\n",
    "personal_best_error=np.full((1, 10), 50).astype(float)\n",
    "personal_best_error=personal_best_error[0][:]\n",
    "global_best_position=[]\n",
    "global_best_error=100  ##we set this to 100 so we are sure that the algorithm will find a smaller value\n",
    "personal_best_position=np.zeros(120).reshape(10,3,4)\n",
    "\n",
    "alpha=1.49618\n",
    "w=0.7298\n",
    "velocity=np.zeros(12).reshape(3,4) \n",
    "\n",
    "##for each iteration\n",
    "for iteration in range(0,20):\n",
    "    \n",
    "    quantization_error=[] \n",
    "    count=0\n",
    "    \n",
    "    ##for each particles \n",
    "    for i in range(0,len(particles_list_iris)):\n",
    "\n",
    "        \n",
    "        current_particle=particles_list_iris[i]\n",
    "        cluster1=[]\n",
    "        cluster2=[]\n",
    "        cluster3=[]\n",
    "        \n",
    "       ##for each data point in the Artificial dataset\n",
    "        for a in range(0,len(iris_df)):\n",
    "            \n",
    "            \n",
    "            ##we measure the euclidean distance from each centroid\n",
    "            dist1=scipy.spatial.distance.euclidean(np.asarray(iris_df.loc[a]), current_particle[0])\n",
    "            dist2=scipy.spatial.distance.euclidean(np.asarray(iris_df.loc[a]), current_particle[1])\n",
    "            dist3=scipy.spatial.distance.euclidean(np.asarray(iris_df.loc[a]), current_particle[2])\n",
    "            \n",
    "            \n",
    "            ##we assign each datapoint to a cluster based on the closest centroid\n",
    "            if dist1<dist2 and dist1<dist3:\n",
    "                cluster1.append(iris_df.loc[a])\n",
    "\n",
    "\n",
    "            elif dist2<dist3 and dist2<dist3:\n",
    "                cluster2.append(iris_df.loc[a])\n",
    "                \n",
    "            elif dist3<dist1 and dist3<dist1:\n",
    "                cluster3.append(iris_df.loc[a])\n",
    "\n",
    "\n",
    "        \n",
    "        ##Once we have our cluster we compute the quantization error and we store it\n",
    "        ## we will have a list with all our quantization error\n",
    "        quantization_error.append(quantization_error3(cluster1,cluster2,cluster3,current_particle[0],current_particle[1],current_particle[2]))\n",
    "\n",
    "   \n",
    "    \n",
    "    ##Then for each quantization error in the list\n",
    "    for z in range(0,len(quantization_error)):\n",
    "        count=count+1    \n",
    "        \n",
    "        ##if the quantization error of the current iteration is smaller than the personal best , we store it as new personal best\n",
    "        if quantization_error[z]<personal_best_error[z]:\n",
    "            personal_best_error[z]=quantization_error[z]\n",
    "            ##and we store the corresponding particle as new best position\n",
    "            personal_best_position[z]=particles_list_iris[z]\n",
    "\n",
    "        ##we also update the best global error if we find a new best \n",
    "        if personal_best_error[z]<global_best_error:\n",
    "            global_best_error=personal_best_error[z]\n",
    "            global_best_position=particles_list_iris[z]\n",
    "            \n",
    "    ###print (\"Particle list BEFORE, \", particles_list)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ## then for each particle we update the position of the centroids based on the formula\n",
    "    \n",
    "    for i in range(0,len(particles_list_iris)):\n",
    "        \n",
    "        ##I've disassembled the formula for clarity.\n",
    "\n",
    "        \n",
    "\n",
    "        ##we set the random r\n",
    "        r1=np.random.uniform(0,1,1)\n",
    "        r2=np.random.uniform(0,1,1)\n",
    "\n",
    "        ##we compute the first term of the formula\n",
    "        first_term=np.multiply(w,velocity)\n",
    "        \n",
    "        ##we compute the two multiplication of the alpha and the r\n",
    "        alphaR1=alpha*r1\n",
    "        alphaR2=alpha*r2\n",
    "        \n",
    "        #we copmute the 2 subtractions\n",
    "        first_subtraction=np.subtract(personal_best_position[i],particles_list_iris[i])\n",
    "        second_subtraction=np.subtract(global_best_position,particles_list_iris[i])\n",
    "\n",
    "        #we compute the second term\n",
    "        second_term=np.multiply(alphaR1,first_subtraction)\n",
    "        \n",
    "        #we compute the second term\n",
    "        third_term=np.multiply(alphaR2,second_subtraction)\n",
    "        \n",
    "        ##we compute the velocity\n",
    "        velocity=np.add(first_term,second_term)\n",
    "        velocity=np.add(velocity,third_term)\n",
    "\n",
    "        #we update the position of the centroids of the particles\n",
    "        particles_list_iris[i]=np.add(particles_list_iris[i],velocity)\n",
    "        \n",
    "    ##print (\"global best error is: \", global_best_error)\n",
    "    ##print (\"global best position: \", global_best_position)\n",
    "    ##print (\"personal best error: \", personal_best_error)\n",
    "    ##print (\"Particle list AFTER\", particles_list)\n",
    "    ##print(\"personal best position  \",personal_best_position )\n",
    "\n",
    "    \n",
    "print(\"After \",iteration,\" iteration , the global best error is: \", global_best_error, \"best  swarn has positions: \", global_best_position)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we compare the results with the results described in the paper, we can notice that our PSO and KMEANS algorithm performs somehow similarly to the results of the paper. In fact we have:\n",
    "\n",
    "- Quantization error for the KMEans algorithm for the articifial dataset:   0.5778295493707196\n",
    "- Quantization error for the KMEans algorithm for the iris dataset:   0.6473743892616001\n",
    "\n",
    "- Quantization error for the PSO algorithm for the articifial dataset:  0.4570523934830559 \n",
    "- Quantization error for the PSO algorithm for the articifial dataset::   0.8999702242159057 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4\n",
    "\n",
    "The figure shows an example from the ACO book by Dorigo and Stuetzle. What results do you expect for an ant colony algorithm that does not use tabu lists (except for inhibition of immediate return to the previous node)?\n",
    "\n",
    "<img src=\"ACO.png\" alt=\"Drawing\" style=\"width: 30%;\"/>\n",
    "\n",
    "> ACO is inspired from the foraging behaviour of ants. How they find the closest food sources to their nests. They communicate with pheromones they leave. They leave their pheromone while moving. Also they tend to choose paths marked by strong pheromone concentrations. When ant finds a food source, it leaves pheromones according to some probability that may depend on the quantity and quality of the food during the return trip. Thus, quality food sources closest to the nest will have the path with more pheromone. \n",
    "However in this case without a tabu list, it is unavoidable that ants make cycles. Bottom of the picture, there are a lot of connected paths and this will cause a lot of pheromone accumulation and this result with most likely loopy paths. For the graph like this converging the minumum path is not trivial.\n",
    "To find the optimum solution an ant has to make number of correct desicions otherwise ant generates suboptimal solutions.\n",
    "There is a trade off between use of an easy but suboptimal path and searching the optimal path. \n",
    "In this case easy path would be that the ants takes the route trough the upper nodes and there is two optimal paths which are in the lower part of the graph and harder to find.\n",
    "We can avoid from cycles with high pheromone evaporation However, this will end up with not generating optimum solutions. Since this problem is relatively simple problem because of the large number of ants compared to the relatively small search space. After a lot of iterations algorithm find the fastest route in the densely connected part of graph which is down-up-right-down-up or down-down-rigt-up-up.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5\n",
    "\n",
    "Assume that ants are allowed to lay pheromone on a path at every time step, so that the pheromone update rule is applied at each time step. Come up with a combination local/global updating scheme that encourages exploration and exploitation - consider which parameters influence this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Ant is probabilisticaly choose the next node based on the amount of pheromone on the next node while it is searching for food exploring the space.\n",
    "For each ant we can keep a limited memory which is local pheromone this allows them to record their trip to their destination. When they reach their destination they can construct a way to back home. If they've passed over the same node twice they can eliminate the path in between those repeat nodes. Then they will retrace the steps they constructed based on the memory(local pheromone). On the way back to the nest ant will ignore the pheromone trails, follow the path that is costructed.\n",
    "When they are going back to the nest they will be applying pheromones to the ground(global pheromones) they will not apply to the global pheromones on the food searching step because that can cause loops.\n",
    "After each round the new pheromones are added to the old ones. For better exploitation, good ants should influence the new pheromones more than others.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
